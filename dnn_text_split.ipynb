{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.all import *\n",
    "\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24136, 7)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#加载数据集\n",
    "x_train, y_train, x_test, y_test = load_dataset('d:/data_3.csv', train_size = 0.8)\n",
    "#特征向量维度\n",
    "m_samples, n_features = x_train.shape\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#网络结构\n",
    "_x = tf.placeholder(tf.float32, shape=(None, n_features), name='input')\n",
    "_y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "L1 = tf.layers.Dense(10, activation=tf.nn.relu)(_x)\n",
    "L2 = tf.layers.Dense(5, activation=tf.nn.relu)(L1)\n",
    "L3 = tf.layers.Dense(1)(L2)\n",
    "logits = tf.nn.sigmoid(L3, name='output')\n",
    "y_predict = tf.where(logits < 0.5, tf.zeros_like(logits), tf.ones_like(logits), name='y_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=_y, logits=L3))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision_op = 1 - tf.reduce_mean(tf.abs(y_predict - _y))\n",
    "c = _y * y_predict\n",
    "accuracy_op = tf.count_nonzero(c) / tf.count_nonzero(y_predict)\n",
    "recall_op = tf.count_nonzero(c) / tf.count_nonzero(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2148, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11 = pd.read_csv('d:/data_4.csv')\n",
    "data1 = data11.as_matrix()\n",
    "m_samples1, n_features1 = data1.shape\n",
    "x_test1 = data1[:, 1:n_features1 - 1]\n",
    "y_test1 = data1[:, n_features1 - 1:n_features1]\n",
    "x_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0001 loss:  0.6775 acc:  0.9520 loss_test:  0.6774 acc_test:  0.9484\n",
      "epoch:  0101 loss:  0.3421 acc:  0.9793 loss_test:  0.3421 acc_test:  0.9791\n",
      "epoch:  0201 loss:  0.1308 acc:  0.9793 loss_test:  0.1303 acc_test:  0.9791\n",
      "epoch:  0301 loss:  0.0731 acc:  0.9793 loss_test:  0.0726 acc_test:  0.9791\n",
      "epoch:  0401 loss:  0.0497 acc:  0.9793 loss_test:  0.0493 acc_test:  0.9791\n",
      "epoch:  0501 loss:  0.0393 acc:  0.9794 loss_test:  0.0390 acc_test:  0.9797\n",
      "epoch:  0601 loss:  0.0339 acc:  0.9767 loss_test:  0.0337 acc_test:  0.9762\n",
      "epoch:  0701 loss:  0.0308 acc:  0.9810 loss_test:  0.0305 acc_test:  0.9818\n",
      "epoch:  0801 loss:  0.0286 acc:  0.9852 loss_test:  0.0283 acc_test:  0.9855\n",
      "epoch:  0901 loss:  0.0272 acc:  0.9873 loss_test:  0.0269 acc_test:  0.9870\n",
      "epoch:  1001 loss:  0.0263 acc:  0.9874 loss_test:  0.0260 acc_test:  0.9870\n",
      "epoch:  1101 loss:  0.0256 acc:  0.9874 loss_test:  0.0253 acc_test:  0.9872\n",
      "epoch:  1201 loss:  0.0250 acc:  0.9874 loss_test:  0.0247 acc_test:  0.9872\n",
      "epoch:  1301 loss:  0.0243 acc:  0.9874 loss_test:  0.0240 acc_test:  0.9872\n",
      "epoch:  1401 loss:  0.0236 acc:  0.9874 loss_test:  0.0233 acc_test:  0.9872\n",
      "[0.627677100494234, 0.9548872180451128]\n",
      "[0.6274509803921569, 0.9504950495049505]\n",
      "[0.9958246346555324, 0.954]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init_op)\n",
    "    for i in range(1500):\n",
    "        session.run(train_op, feed_dict={_x: x_train, _y: y_train})\n",
    "        loss, precision = session.run([loss_op, precision_op], feed_dict={_x: x_train, _y: y_train})\n",
    "        loss_test, precision_test = session.run([loss_op, precision_op], feed_dict={_x: x_test, _y: y_test})\n",
    "        if i % 100 == 0:\n",
    "            print('epoch: ', '%04d' % (i + 1), 'loss: ', \"{:.4f}\".format(loss), 'acc: ', \"{:.4f}\".format(precision),\n",
    "                  'loss_test: ', \"{:.4f}\".format(loss_test), 'acc_test: ', \"{:.4f}\".format(precision_test))\n",
    "    print(session.run([accuracy_op, recall_op], feed_dict={_x: x_train, _y: y_train}))\n",
    "    print(session.run([accuracy_op, recall_op], feed_dict={_x: x_test, _y: y_test}))\n",
    "    print(session.run([accuracy_op, recall_op], feed_dict={_x: x_test1, _y: y_test1}))\n",
    "    #保存模型\n",
    "#     save_model(session, 'd:/tmp/m_full_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
