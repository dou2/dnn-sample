{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.externals import joblib\n",
    "import keras\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.word2vec_fast import *\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9648, 300)\n"
     ]
    }
   ],
   "source": [
    "question_path = '../data/chip2018/question_id.csv'\n",
    "wv = Word2VecFast.load_word2vec_format(file_path='../data/chip2018/word_embedding.txt', word_shape=300)\n",
    "print(wv.word_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 45 # 句子最大长度\n",
    "embedding_size = 300\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "num_classes = 2\n",
    "num_epoch = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载问题\n",
    "question_c_dict = {}\n",
    "def load_question():\n",
    "    temp1 = {}\n",
    "    with open(file=question_path, encoding='utf-8') as f:\n",
    "        for line_num,line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                continue\n",
    "            ss = line.split(',')\n",
    "            qid,qw,qc = ss[0],ss[1],ss[2]\n",
    "            temp1[qid] = wv.words_id(qw)\n",
    "    return temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2466, 8275, 913, 2369, 9586, 9527, 8479, 8275, 805, 586]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_w_dict = load_question()\n",
    "question_w_dict['S100012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x_,y_ = [],[]\n",
    "    with open(file=train_path, encoding='utf-8') as f:\n",
    "        for line_num,line in enumerate(f):\n",
    "            if line_num == 0:\n",
    "                continue\n",
    "            ss = line.split(',')\n",
    "            a1 = question_w_dict[ss[0]]\n",
    "            a2 = question_w_dict[ss[1]]\n",
    "            a1 = sequence.pad_sequences([a1], maxlen=sequence_length, padding='post', value=wv.words_id('<UNK>'))[0]\n",
    "            a2 = sequence.pad_sequences([a2], maxlen=sequence_length, padding='post', value=wv.words_id('<UNK>'))[0]\n",
    "            a = np.hstack((a1, a2))\n",
    "            x_.append(a)\n",
    "            y_.append(ss[2])\n",
    "    return x_,y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 90)\n"
     ]
    }
   ],
   "source": [
    "train_path = '../data/chip2018/train.csv'\n",
    "X1_, Y_ = load_dataset()\n",
    "X1_ = np.array(X1_)\n",
    "Y_ = np.array(Y_)\n",
    "print(X1_.shape)\n",
    "num_examples = X1_.shape[0]\n",
    "total_batch = int(num_examples/batch_size)\n",
    "Y_ = keras.utils.to_categorical(Y_, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(?, 45, 300, 1), dtype=float32)\n",
      "Tensor(\"ExpandDims_1:0\", shape=(?, 45, 300, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 嵌入层\n",
    "WL_word2vec = tf.constant(wv.word_embeddings(), dtype=tf.float32)\n",
    "WL_word_embedding1 = tf.nn.embedding_lookup(WL_word2vec, input_x1)\n",
    "WL_word_embedding_expanded1 = tf.expand_dims(WL_word_embedding1, -1)\n",
    "print(WL_word_embedding_expanded1)\n",
    "\n",
    "# 嵌入层\n",
    "WL_word_embedding2 = tf.nn.embedding_lookup(WL_word2vec, input_x2)\n",
    "WL_word_embedding_expanded2 = tf.expand_dims(WL_word_embedding2, -1)\n",
    "print(WL_word_embedding_expanded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 1, 1, 128), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 1, 1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([3, embedding_size, 1, num_filters], stddev=0.1), dtype=tf.float32)\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "\n",
    "conv1 = tf.nn.conv2d(input=WL_word_embedding_expanded1, filter=W1, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "h1 = tf.nn.relu(tf.nn.bias_add(conv1, b1))\n",
    "pooled1 = tf.nn.max_pool(h1, ksize=[1, sequence_length - 3 + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "print(pooled1)\n",
    "\n",
    "conv2 = tf.nn.conv2d(input=WL_word_embedding_expanded2, filter=W1, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "h2 = tf.nn.relu(tf.nn.bias_add(conv2, b1))\n",
    "pooled2 = tf.nn.max_pool(h2, ksize=[1, sequence_length - 3 + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "print(pooled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 128), dtype=float32) Tensor(\"Reshape_1:0\", shape=(?, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h_pool_flat1 = tf.reshape(pooled1, [-1, num_filters])\n",
    "h_pool_flat2 = tf.reshape(pooled2, [-1, num_filters])\n",
    "print(h_pool_flat1, h_pool_flat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv:0\", shape=(?, 128), dtype=float32) Tensor(\"truediv_1:0\", shape=(?, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h_pool_flat1 = h_pool_flat1 / tf.norm(h_pool_flat1)\n",
    "h_pool_flat2 = h_pool_flat2 / tf.norm(h_pool_flat2)\n",
    "print(h_pool_flat1, h_pool_flat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h_drop = tf.multiply(h_pool_flat1, h_pool_flat2)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([num_filters, num_classes], stddev=0.1))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "logits = tf.nn.xw_plus_b(h_drop, W2, b2)\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "correct_predict = tf.equal(tf.argmax(predictions, axis=1), tf.argmax(input_y, axis=1))\n",
    "acc_op = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=input_y))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(loss_op)\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(i):\n",
    "    a = i * batch_size\n",
    "    b = (i+1) * batch_size\n",
    "    return (X1_[a:b], Y_[a:b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss= 0.693160074 accuracy= 0.496844952\n",
      "Epoch: 0002 loss= 0.693095208 accuracy= 0.502153446\n",
      "Epoch: 0003 loss= 0.692754656 accuracy= 0.512419872\n",
      "Epoch: 0004 loss= 0.692329844 accuracy= 0.517678285\n",
      "Epoch: 0005 loss= 0.691820627 accuracy= 0.514823718\n",
      "Epoch: 0006 loss= 0.691369498 accuracy= 0.513872196\n",
      "Epoch: 0007 loss= 0.691059194 accuracy= 0.515825321\n",
      "Epoch: 0008 loss= 0.690801490 accuracy= 0.515675080\n",
      "Epoch: 0009 loss= 0.690573449 accuracy= 0.515324519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ab1adc630c54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_x1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_x2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minput_x1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_x2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        avg_cost = 0.\n",
    "        avg_acc = 0.\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = next_batch(i)\n",
    "            sess.run(train_op, feed_dict={input_x1: batch_xs[:,0:45], input_x2: batch_xs[:,45:], input_y: batch_ys, dropout_keep_prob: 0.5})\n",
    "            c, acc = sess.run([loss_op, acc_op], feed_dict={input_x1: batch_xs[:,0:45], input_x2: batch_xs[:,45:], input_y: batch_ys, dropout_keep_prob: 0.5})\n",
    "            avg_cost += c / total_batch\n",
    "            avg_acc += acc / total_batch\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_cost), \"accuracy=\", \"{:.9f}\".format(avg_acc))\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
